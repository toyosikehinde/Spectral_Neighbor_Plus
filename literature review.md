**Literature Review: Lyrical Semantics and Multimodal Music Understanding**

A central body of work that informs Spectral Neighbor Plus examines how musical meaning emerges from the interaction of audio content and lyrical text. Two studies are particularly significant. The first is the work of Laurier, Grivolla and Herrera on multimodal mood classification, and the second is the work of Fell and Sporleder on the linguistic structure and stylistic identity of lyrics. Together, these studies show that musical similarity cannot be captured through acoustic features alone. Instead, it must also engage with semantic patterns, textual rhythm, vocabulary, and stylistic conventions. These ideas provide the conceptual foundation for extending my system beyond pure spectral analysis toward a more complete and human centered representation of musical identity.

Laurier and colleagues begin from the idea that musical mood arises from a combination of what a piece sounds like and what it communicates verbally. They model mood using several binary emotional labels such as angry, happy, sad and relaxed, which they treat as independent supervised learning problems. Their audio analysis pipeline closely resembles the one that powers the earlier version of the Spectral Neighbor prototype. They extract mel frequency cepstral coefficients and related spectral measures, along with descriptors of brightness, timbral shape, tempo and rhythmic energy. All features are computed from the short time Fourier transform and are summarized across time to produce a compact representation for each song. These acoustic features capture many of the signals listeners associate with excitement or calmness, and their experiments confirm that audio alone is especially effective for high arousal categories. At the same time, they observe clear limitations. Valence, a more subtle emotional quality, does not reliably emerge from spectral information. This gap points toward the need for textual information to reveal aspects of emotional meaning that audio features cannot supply.

Their lyrical analysis explores several approaches. They begin with a simple term frequency and inverse document frequency representation. They then apply latent semantic analysis to discover broader themes across songs. However, they achieve the strongest results when they build language models that estimate how the probability of specific terms changes between mood categories and non mood categories. This highlights terms that serve as emotional markers, both positive and negative. Words associated with intimacy, heartbreak, confrontation or affection distribute differently across categories and provide strong cues for emotional interpretation. When these lyrical features are combined with the spectral features, the models perform better than either modality alone. The improvement is especially large for categories such as happy and sad, which are strongly tied to valence. This confirms that lyrics provide semantic information that stabilizes and enriches the emotional signal drawn from the audio space.

While Laurier and colleagues focus primarily on affect, the work of Fell and Sporleder turns toward genre and stylistic identity within lyrics themselves. Their study argues that lyrics behave in many ways like poetic texts. They contain metaphor, figurative language, repeated phrasing, distinctive grammatical structures and patterns of rhyme that reflect cultural characteristics of each genre. They analyze lexical richness, readability, typical sentence structure and the distribution of parts of speech. These measures capture how complex or simple the language tends to be and whether the lyrics rely on direct address, narrative storytelling or abstract description. They also treat rhyme as a vital source of stylistic and rhythmic information, measuring the frequency and diversity of rhyme endings across songs. Genres such as rap and certain styles of metal show dense and varied rhyme structures, while pop and dance oriented music exhibit simpler repetitive patterns. Their findings show that these stylistic features are powerful indicators of genre on their own and become even more effective when paired with the semantic information that comes from vocabulary patterns.

The most important implication for my system is that lyrics encode both meaning and structure. Vocabulary and thematic content provide insight into emotion and narrative, while rhyme and line layout create a form of textual rhythm that mirrors aspects of musical rhythm. This means that lyrics can supply information parallel to audio features rather than merely supplementing them. The combination of stylometric cues and lexical semantics allows a model to describe how a song communicates as well as what it communicates.

Together, these two bodies of work support the central design choice in Spectral Neighbor Plus. Audio features capture timbre, rhythmic motion and energy, while lyrics reveal emotional meaning, cultural context, narrative themes and stylistic nuance. Incorporating both perspectives allows the system to represent music in a more human aligned way. Instead of relying exclusively on sound based similarity, the recommender can account for how a song feels, how it communicates ideas and how it embodies the stylistic conventions of its genre. This produces a richer neighborhood structure that better reflects how people naturally compare and relate songs.
